{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "191fb2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5ffbf85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "# For text preprocessing\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from wordsegment import segment, load\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "sys.setrecursionlimit(10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "53f794d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af6c089",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f4420bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text, tknzr):\n",
    "    FLAGS = re.MULTILINE | re.DOTALL\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "\n",
    "    # function so code less repetitive\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
    "    text = re_sub(r\"/\",\" / \")\n",
    "    text = re_sub(r\"@\\w+\", \"<user>\")\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
    "    text = re_sub(r\"<3\",\"<heart>\")\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
    "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
    "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
    "    # print(text)\n",
    "    text = re_sub(r\"#\\S+\", lambda hashtag: \" \".join(segment(hashtag.group()[1:]))) # segment hastags\n",
    "    # text = text.replace('#','')\n",
    "    # print(text)\n",
    "    # exit()\n",
    "\n",
    "    tokens = tknzr.tokenize(text.lower())\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def concat_data(id2entities):\n",
    "    # \twith open(dir_path+name, \"rb\") as f:\n",
    "    # \t\tid2entities = pickle.load(f)\n",
    "\n",
    "    ########## Lookup Tables ##########\n",
    "    labels = list(set([entity[0] for entity in id2entities.values()]))\n",
    "    num_classes = len(labels)\n",
    "    \n",
    "    \n",
    "\n",
    "    label_lookup = np.zeros((num_classes,num_classes),int)\n",
    "    np.fill_diagonal(label_lookup, 1)\n",
    "    ###################################\n",
    "\n",
    "    text_data, context_data, label_data = [], [], []\n",
    "    label_dict = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        label_dict[label] = i\n",
    "\n",
    "    # \tload()\n",
    "    tknzr = TweetTokenizer(reduce_len=True, preserve_case=False, strip_handles=False)\n",
    "    # \tprint(\"Preprocessing tweets.....\")\n",
    "    for _id in tqdm(id2entities):\n",
    "        if id2entities[_id][0] in label_dict.keys():\n",
    "            text_data.append(text_preprocess(id2entities[_id][1], tknzr))\n",
    "            context_data.append(text_preprocess(id2entities[_id][2], tknzr))\n",
    "\n",
    "            label_data.append(label_lookup[ label_dict[id2entities[_id][0]] ])\n",
    "\n",
    "    assert len(text_data) == len(context_data) == len(label_data)\n",
    "\n",
    "    return text_data, context_data, label_data,label_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d3827b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pandas as pd\n",
    "import functools\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "# import helper\n",
    "import time\n",
    "\n",
    "import argparse\n",
    "import re\n",
    "\n",
    "\n",
    "def preprocessSplit(processed_dict,lang):\n",
    "    _text, _ctxt, _label,label_dict = concat_data(processed_dict)\n",
    "    text_train, text_test, label_train, label_test = train_test_split(_text, _label,\n",
    "                                                    stratify=_label,\n",
    "                                                    test_size=0.2)\n",
    "    \n",
    "    _data = {\"text_train\": text_train,\n",
    "                     \"label_train\": label_train,\n",
    "                     \"text_test\": text_test,\n",
    "                     \"label_test\": label_test,\n",
    "                     \"label_dict\":label_dict\n",
    "            }\n",
    "    \n",
    "    with open(f\"processed_data/{lang}_processed.pkl\", \"wb\") as f:\n",
    "        pickle.dump(_data, f)\n",
    "        \n",
    "    print(label_dict)\n",
    "        \n",
    "    return _data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5ddfa362",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_class_dict ={'abusive':'offensive',\n",
    " 'hate':'offensive',\n",
    " 'hateful':'offensive',\n",
    " 'neither':'not_offensive',\n",
    " 'non-sexist':'not_offensive',\n",
    " 'none':'not_offensive',\n",
    " 'normal':'not_offensive',\n",
    " 'not_hate':'not_offensive',\n",
    " 'offensive':'offensive',\n",
    " 'sexist':'offensive'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85897598",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fb9daa",
   "metadata": {},
   "source": [
    "## Loading Data and Formatting for Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f069483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a7bd83f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLimited(pd,ratio=3):\n",
    "    keys = random.sample(list(pd), 8000)\n",
    "    nc = {}\n",
    "    for c,k in enumerate(keys):\n",
    "        nc[c] = pd[k]\n",
    "    return nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d0b018fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration mrpc\n",
      "Reusing dataset hate_speech_offensive (/Users/rehanahmed/.cache/huggingface/datasets/hate_speech_offensive/mrpc/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59df425f2dd544e49f040e961726a89d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24783 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bbf3257cc794af6bfc124a27cc5ecb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24783 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'offensive': 0, 'not_offensive': 1}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('hate_speech_offensive', 'mrpc', split='train')\n",
    "\n",
    "class_list = []\n",
    "for i in dataset:\n",
    "    class_list.append(i['class'])\n",
    "\n",
    "pd.Series(class_list).value_counts()\n",
    "\n",
    "processed_dict = {}\n",
    "\n",
    "class_dict = {0:'hate',1:'offensive',2:'neither'}\n",
    "\n",
    "labels = []\n",
    "\n",
    "for c,i in tqdm(enumerate(dataset),total=len(dataset)):\n",
    "    processed_dict[c] = [all_class_dict[class_dict[i['class']]],i['tweet'],'']\n",
    "    \n",
    "    labels.append(class_dict[i['class']])\n",
    "\n",
    "# Preprocess + Train test Split + Save to file\n",
    "spl = preprocessSplit(processed_dict,'english_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5551f7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24783"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "607d895f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "offensive    19190\n",
       "neither       4163\n",
       "hate          1430\n",
       "dtype: int64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(labels).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711902c3",
   "metadata": {},
   "source": [
    "# Filipino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8a6b9780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration mrpc\n",
      "Reusing dataset hate_speech_filipino (/Users/rehanahmed/.cache/huggingface/datasets/hate_speech_filipino/mrpc/1.0.0/89001ab1965f35d6d74585e59f982bbdd09c82a645bf702f32a52ad95404dd83)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c92d5c97b74d6fa3a0b96fba80f964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee5fe67566e49e3831bfb7cd23e85bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1081b192c8c48c98f9f0fdfeedf1f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24232 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'offensive': 0, 'not_offensive': 1}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('hate_speech_filipino', 'mrpc')\n",
    "\n",
    "class_list = []\n",
    "for j in ['train','test','validation']:\n",
    "    for i in dataset[j]:\n",
    "        class_list.append(i['label'])\n",
    "\n",
    "pd.Series(class_list).value_counts()\n",
    "\n",
    "processed_dict = {}\n",
    "\n",
    "class_dict = {0:'not_hate',1:'hate'}\n",
    "\n",
    "c=0\n",
    "labels = []\n",
    "for j in tqdm(['train','test','validation']):\n",
    "    for i in (dataset[j]):\n",
    "        processed_dict[c] = [all_class_dict[class_dict[i['label']]],i['text'],'']\n",
    "        c = c+1\n",
    "        labels.append(class_dict[i['label']])\n",
    "        \n",
    "    \n",
    "sp = preprocessSplit(processed_dict,'filipino')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "54490596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24232"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "16dab478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "not_hate    12979\n",
       "hate        11253\n",
       "dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(labels).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c7ad45",
   "metadata": {},
   "source": [
    "# Chinese "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7d7180b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ls = ['<url>','<user>','<smile>','<lolface>','<sadface>','<neutralface>','<heart>','<number>','<repeat>','<elong>']\n",
    "e_ls = list(map(lambda a:a.replace('<','').replace('>',''),my_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "555bc561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text, tknzr):\n",
    "    FLAGS = re.MULTILINE | re.DOTALL\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "\n",
    "    # function so code less repetitive\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
    "    text = re_sub(r\"/\",\" / \")\n",
    "    text = re_sub(r\"@\\w+\", \"<user>\")\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
    "    text = re_sub(r\"<3\",\"<heart>\")\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
    "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
    "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
    "    # print(text)\n",
    "    text = re_sub(r\"#\\S+\", lambda hashtag: \" \".join(segment(hashtag.group()[1:]))) # segment hastags\n",
    "    # text = text.replace('#','')\n",
    "    # print(text)\n",
    "    # exit()\n",
    "\n",
    "#     tokens = tknzr.tokenize(text.lower())\n",
    "    tokens = jieba.lcut(text, cut_all=False)\n",
    "    ret_text = \" \".join(tokens)\n",
    "\n",
    "    for i in e_ls:\n",
    "        if i in ret_text:\n",
    "            ret_text = ret_text.replace('< '+i+' >','<'+i+'>')\n",
    "\n",
    "\n",
    "    return ret_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558714da",
   "metadata": {},
   "source": [
    "### Place file SexComment.csv in Raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "70a64368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SexComment.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls raw_datasets | grep SexComment.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "55e09e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0bd4398fce4b569d955461ca21ab27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2410c65006940b8ba2296f4d4450598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8969 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'offensive': 0, 'not_offensive': 1}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('raw_datasets/SexComment.csv')\n",
    "\n",
    "df.head()\n",
    "\n",
    "processed_dict = {}\n",
    "\n",
    "labels = []\n",
    "\n",
    "class_dict = {0:'non-sexist',1:'sexist'}\n",
    "\n",
    "for index,row in tqdm(df.iterrows()):\n",
    "    processed_dict[index] = [all_class_dict[class_dict[row['label']]],row['comment_text'],'']\n",
    "    labels.append(class_dict[row['label']])\n",
    "\n",
    "    \n",
    "# processed_dict =   getLimited(processed_dict)\n",
    "\n",
    "sp = preprocessSplit(processed_dict,'chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bafcf09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8969"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dab5e6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "non-sexist    5876\n",
       "sexist        3093\n",
       "dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(labels).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2eef86",
   "metadata": {},
   "source": [
    "# Korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "168754ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text, tknzr):\n",
    "    FLAGS = re.MULTILINE | re.DOTALL\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "\n",
    "    # function so code less repetitive\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
    "    text = re_sub(r\"/\",\" / \")\n",
    "    text = re_sub(r\"@\\w+\", \"<user>\")\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
    "    text = re_sub(r\"<3\",\"<heart>\")\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
    "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
    "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
    "    \n",
    "    # print(text)\n",
    "    text = re_sub(r\"#\\S+\", lambda hashtag: \" \".join(segment(hashtag.group()[1:]))) # segment hastags\n",
    "    # text = text.replace('#','')\n",
    "    # print(text)\n",
    "    # exit()\n",
    "\n",
    "    # tokens = tknzr.tokenize(text.lower())\n",
    "    tokens = hannanum.morphs(text)\n",
    "    ret_text = \" \".join(tokens)\n",
    "\n",
    "    for i in e_ls:\n",
    "        if i in ret_text:\n",
    "            ret_text = ret_text.replace('< '+i+' >','<'+i+'>')\n",
    "\n",
    "\n",
    "    return ret_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "282efa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import koco\n",
    "\n",
    "train_dev = koco.load_dataset('korean-hate-speech', mode='train_dev')\n",
    "# test_dev = koco.load_dataset('korean-hate-speech', mode='test')\n",
    "\n",
    "processed_dict = {}\n",
    "\n",
    "labels = []\n",
    "\n",
    "c = 0\n",
    "for i in train_dev['train']:\n",
    "    processed_dict[c] = [all_class_dict[i['hate']],i['comments'],'']\n",
    "    c=c+1\n",
    "    labels.append(i['hate'])\n",
    "for i in train_dev['dev']:\n",
    "    processed_dict[c] = [all_class_dict[i['hate']],i['comments'],'']\n",
    "    c=c+1\n",
    "    labels.append(i['hate'])\n",
    "\n",
    "# sp = preprocessSplit(processed_dict,'korean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d5f23b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8367"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e7dc5920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "none         3646\n",
       "offensive    2688\n",
       "hate         2033\n",
       "dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(labels).value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
