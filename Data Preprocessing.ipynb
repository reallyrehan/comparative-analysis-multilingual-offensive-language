{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "191fb2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5ffbf85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "# For text preprocessing\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from wordsegment import segment, load\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "sys.setrecursionlimit(10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "53f794d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af6c089",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f4420bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text, tknzr):\n",
    "    FLAGS = re.MULTILINE | re.DOTALL\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "\n",
    "    # function so code less repetitive\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
    "    text = re_sub(r\"/\",\" / \")\n",
    "    text = re_sub(r\"@\\w+\", \"<user>\")\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
    "    text = re_sub(r\"<3\",\"<heart>\")\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
    "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
    "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
    "    # print(text)\n",
    "    text = re_sub(r\"#\\S+\", lambda hashtag: \" \".join(segment(hashtag.group()[1:]))) # segment hastags\n",
    "    # text = text.replace('#','')\n",
    "    # print(text)\n",
    "    # exit()\n",
    "\n",
    "    tokens = tknzr.tokenize(text.lower())\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def concat_data(id2entities):\n",
    "    # \twith open(dir_path+name, \"rb\") as f:\n",
    "    # \t\tid2entities = pickle.load(f)\n",
    "\n",
    "    ########## Lookup Tables ##########\n",
    "    labels = list(set([entity[0] for entity in id2entities.values()]))\n",
    "    num_classes = len(labels)\n",
    "    \n",
    "    \n",
    "\n",
    "    label_lookup = np.zeros((num_classes,num_classes),int)\n",
    "    np.fill_diagonal(label_lookup, 1)\n",
    "    ###################################\n",
    "\n",
    "    text_data, context_data, label_data = [], [], []\n",
    "    label_dict = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        label_dict[label] = i\n",
    "\n",
    "    # \tload()\n",
    "    tknzr = TweetTokenizer(reduce_len=True, preserve_case=False, strip_handles=False)\n",
    "    # \tprint(\"Preprocessing tweets.....\")\n",
    "    for _id in tqdm(id2entities):\n",
    "        if id2entities[_id][0] in label_dict.keys():\n",
    "            text_data.append(text_preprocess(id2entities[_id][1], tknzr))\n",
    "            context_data.append(text_preprocess(id2entities[_id][2], tknzr))\n",
    "\n",
    "            label_data.append(label_lookup[ label_dict[id2entities[_id][0]] ])\n",
    "\n",
    "    assert len(text_data) == len(context_data) == len(label_data)\n",
    "\n",
    "    return text_data, context_data, label_data,label_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d3827b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pandas as pd\n",
    "import functools\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "# import helper\n",
    "import time\n",
    "\n",
    "import argparse\n",
    "import re\n",
    "\n",
    "def train(classifier,n_gram_tuple,feature_level,max_feature_length,_data,labels):\n",
    "    if classifier == 'NB':\n",
    "        text_clf = Pipeline([('vect', TfidfVectorizer(ngram_range=(n_gram_tuple[0],n_gram_tuple[1]), analyzer=feature_level, max_features=max_feature_length)),\n",
    "                        ('clf', MultinomialNB())])\n",
    "\n",
    "    elif classifier == 'LR':\n",
    "        text_clf = Pipeline([('vect', TfidfVectorizer(ngram_range=(n_gram_tuple[0],n_gram_tuple[1]), analyzer=feature_level, max_features=max_feature_length)),\n",
    "                        ('clf', LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\"))])\n",
    "\n",
    "    elif classifier == 'SVM':\n",
    "        text_clf = Pipeline([('vect', TfidfVectorizer(ngram_range=(n_gram_tuple[0],n_gram_tuple[1]), analyzer=feature_level, max_features=max_feature_length)),\n",
    "                        ('clf', SGDClassifier(loss='log', penalty='l2'))])\n",
    "\n",
    "    elif classifier == 'RF':\n",
    "        text_clf = Pipeline([('vect', TfidfVectorizer(ngram_range=(n_gram_tuple[0],n_gram_tuple[1]), analyzer=feature_level, max_features=max_feature_length)),\n",
    "                        ('clf', RandomForestClassifier())]) \n",
    "\n",
    "    elif classifier == 'GBT':\n",
    "        text_clf = Pipeline([('vect', TfidfVectorizer(ngram_range=(n_gram_tuple[0],n_gram_tuple[1]), analyzer=feature_level, max_features=max_feature_length)),\n",
    "                        ('clf', GradientBoostingClassifier(learning_rate=1, max_depth=1))])\n",
    "        \n",
    "    \n",
    "    train_label_data = [i.tolist().index(1) for i in _data['label_train']]\n",
    "    text_clf.fit(_data[\"text_train\"], train_label_data)\n",
    "    \n",
    "    preds = text_clf.predict(_data[\"text_test\"])\n",
    "    \n",
    "    test_label_data = [i.tolist().index(1) for i in _data['label_test']]\n",
    "\n",
    "    print(classifier)\n",
    "#     print(labels)\n",
    "    print(classification_report(test_label_data, preds,target_names=labels))\n",
    "    print(\"======================\")\n",
    "    \n",
    "    return text_clf,test_label_data,preds\n",
    "\n",
    "def preprocessSplit(processed_dict,lang):\n",
    "    _text, _ctxt, _label,label_dict = concat_data(processed_dict)\n",
    "    text_train, text_test, label_train, label_test = train_test_split(_text, _label,\n",
    "                                                    stratify=_label,\n",
    "                                                    test_size=0.2)\n",
    "    \n",
    "    _data = {\"text_train\": text_train,\n",
    "                     \"label_train\": label_train,\n",
    "                     \"text_test\": text_test,\n",
    "                     \"label_test\": label_test,\n",
    "                     \"label_dict\":label_dict\n",
    "            }\n",
    "    \n",
    "    with open(f\"processed_data/{lang}_processed.pkl\", \"wb\") as f:\n",
    "        pickle.dump(_data, f)\n",
    "        \n",
    "    print(label_dict)\n",
    "        \n",
    "    return _data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5ddfa362",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_class_dict ={'abusive':'offensive',\n",
    " 'hate':'offensive',\n",
    " 'hateful':'offensive',\n",
    " 'neither':'not_offensive',\n",
    " 'non-sexist':'not_offensive',\n",
    " 'none':'not_offensive',\n",
    " 'normal':'not_offensive',\n",
    " 'not_hate':'not_offensive',\n",
    " 'offensive':'offensive',\n",
    " 'sexist':'offensive'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85897598",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fb9daa",
   "metadata": {},
   "source": [
    "## Loading Data and Formatting for Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dba71d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f206126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLimited(pd,ratio=3):\n",
    "    keys = random.sample(list(pd), 8000)\n",
    "    nc = {}\n",
    "    for c,k in enumerate(keys):\n",
    "        nc[c] = pd[k]\n",
    "    return nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d0b018fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration mrpc\n",
      "Reusing dataset hate_speech_offensive (/Users/rehanahmed/.cache/huggingface/datasets/hate_speech_offensive/mrpc/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59df425f2dd544e49f040e961726a89d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24783 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bbf3257cc794af6bfc124a27cc5ecb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24783 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'offensive': 0, 'not_offensive': 1}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('hate_speech_offensive', 'mrpc', split='train')\n",
    "\n",
    "class_list = []\n",
    "for i in dataset:\n",
    "    class_list.append(i['class'])\n",
    "\n",
    "pd.Series(class_list).value_counts()\n",
    "\n",
    "processed_dict = {}\n",
    "\n",
    "class_dict = {0:'hate',1:'offensive',2:'neither'}\n",
    "\n",
    "labels = []\n",
    "\n",
    "for c,i in tqdm(enumerate(dataset),total=len(dataset)):\n",
    "    processed_dict[c] = [all_class_dict[class_dict[i['class']]],i['tweet'],'']\n",
    "    \n",
    "    labels.append(class_dict[i['class']])\n",
    "\n",
    "# Preprocess + Train test Split + Save to file\n",
    "spl = preprocessSplit(processed_dict,'english_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "adc06870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24783"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9b68a824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "offensive    19190\n",
       "neither       4163\n",
       "hate          1430\n",
       "dtype: int64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(labels).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711902c3",
   "metadata": {},
   "source": [
    "# Filipino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8a6b9780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration mrpc\n",
      "Reusing dataset hate_speech_filipino (/Users/rehanahmed/.cache/huggingface/datasets/hate_speech_filipino/mrpc/1.0.0/89001ab1965f35d6d74585e59f982bbdd09c82a645bf702f32a52ad95404dd83)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c92d5c97b74d6fa3a0b96fba80f964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee5fe67566e49e3831bfb7cd23e85bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1081b192c8c48c98f9f0fdfeedf1f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24232 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'offensive': 0, 'not_offensive': 1}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('hate_speech_filipino', 'mrpc')\n",
    "\n",
    "class_list = []\n",
    "for j in ['train','test','validation']:\n",
    "    for i in dataset[j]:\n",
    "        class_list.append(i['label'])\n",
    "\n",
    "pd.Series(class_list).value_counts()\n",
    "\n",
    "processed_dict = {}\n",
    "\n",
    "class_dict = {0:'not_hate',1:'hate'}\n",
    "\n",
    "c=0\n",
    "labels = []\n",
    "for j in tqdm(['train','test','validation']):\n",
    "    for i in (dataset[j]):\n",
    "        processed_dict[c] = [all_class_dict[class_dict[i['label']]],i['text'],'']\n",
    "        c = c+1\n",
    "        labels.append(class_dict[i['label']])\n",
    "        \n",
    "    \n",
    "sp = preprocessSplit(processed_dict,'filipino')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e45ca98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24232"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3df0ba45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "not_hate    12979\n",
       "hate        11253\n",
       "dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(labels).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c7ad45",
   "metadata": {},
   "source": [
    "# Chinese "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7d7180b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ls = ['<url>','<user>','<smile>','<lolface>','<sadface>','<neutralface>','<heart>','<number>','<repeat>','<elong>']\n",
    "e_ls = list(map(lambda a:a.replace('<','').replace('>',''),my_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "555bc561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text, tknzr):\n",
    "    FLAGS = re.MULTILINE | re.DOTALL\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "\n",
    "    # function so code less repetitive\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
    "    text = re_sub(r\"/\",\" / \")\n",
    "    text = re_sub(r\"@\\w+\", \"<user>\")\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
    "    text = re_sub(r\"<3\",\"<heart>\")\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
    "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
    "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
    "    # print(text)\n",
    "    text = re_sub(r\"#\\S+\", lambda hashtag: \" \".join(segment(hashtag.group()[1:]))) # segment hastags\n",
    "    # text = text.replace('#','')\n",
    "    # print(text)\n",
    "    # exit()\n",
    "\n",
    "#     tokens = tknzr.tokenize(text.lower())\n",
    "    tokens = jieba.lcut(text, cut_all=False)\n",
    "    ret_text = \" \".join(tokens)\n",
    "\n",
    "    for i in e_ls:\n",
    "        if i in ret_text:\n",
    "            ret_text = ret_text.replace('< '+i+' >','<'+i+'>')\n",
    "\n",
    "\n",
    "    return ret_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faf38e8",
   "metadata": {},
   "source": [
    "### Place file SexComment.csv in Raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6def8704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SexComment.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls raw_datasets | grep SexComment.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "55e09e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0bd4398fce4b569d955461ca21ab27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2410c65006940b8ba2296f4d4450598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8969 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'offensive': 0, 'not_offensive': 1}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('raw_datasets/SexComment.csv')\n",
    "\n",
    "df.head()\n",
    "\n",
    "processed_dict = {}\n",
    "\n",
    "labels = []\n",
    "\n",
    "class_dict = {0:'non-sexist',1:'sexist'}\n",
    "\n",
    "for index,row in tqdm(df.iterrows()):\n",
    "    processed_dict[index] = [all_class_dict[class_dict[row['label']]],row['comment_text'],'']\n",
    "    labels.append(class_dict[row['label']])\n",
    "\n",
    "    \n",
    "# processed_dict =   getLimited(processed_dict)\n",
    "\n",
    "sp = preprocessSplit(processed_dict,'chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9d57822a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8969"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "40677c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "non-sexist    5876\n",
       "sexist        3093\n",
       "dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(labels).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2eef86",
   "metadata": {},
   "source": [
    "# Korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "168754ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text, tknzr):\n",
    "    FLAGS = re.MULTILINE | re.DOTALL\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "\n",
    "    # function so code less repetitive\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
    "    text = re_sub(r\"/\",\" / \")\n",
    "    text = re_sub(r\"@\\w+\", \"<user>\")\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
    "    text = re_sub(r\"<3\",\"<heart>\")\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
    "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
    "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
    "    \n",
    "    # print(text)\n",
    "    text = re_sub(r\"#\\S+\", lambda hashtag: \" \".join(segment(hashtag.group()[1:]))) # segment hastags\n",
    "    # text = text.replace('#','')\n",
    "    # print(text)\n",
    "    # exit()\n",
    "\n",
    "    # tokens = tknzr.tokenize(text.lower())\n",
    "    tokens = hannanum.morphs(text)\n",
    "    ret_text = \" \".join(tokens)\n",
    "\n",
    "    for i in e_ls:\n",
    "        if i in ret_text:\n",
    "            ret_text = ret_text.replace('< '+i+' >','<'+i+'>')\n",
    "\n",
    "\n",
    "    return ret_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "282efa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import koco\n",
    "\n",
    "train_dev = koco.load_dataset('korean-hate-speech', mode='train_dev')\n",
    "# test_dev = koco.load_dataset('korean-hate-speech', mode='test')\n",
    "\n",
    "processed_dict = {}\n",
    "\n",
    "labels = []\n",
    "\n",
    "c = 0\n",
    "for i in train_dev['train']:\n",
    "    processed_dict[c] = [all_class_dict[i['hate']],i['comments'],'']\n",
    "    c=c+1\n",
    "    labels.append(i['hate'])\n",
    "for i in train_dev['dev']:\n",
    "    processed_dict[c] = [all_class_dict[i['hate']],i['comments'],'']\n",
    "    c=c+1\n",
    "    labels.append(i['hate'])\n",
    "\n",
    "# sp = preprocessSplit(processed_dict,'korean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "232051b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8367"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a292e9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "none         3646\n",
       "offensive    2688\n",
       "hate         2033\n",
       "dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcfa1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc439b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fca5e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['NB','LR','SVM','RF','GBT']\n",
    "languages = ['english_1','filipino','chinese','korean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "01fa930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe3bb77",
   "metadata": {},
   "source": [
    "# Total Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c62c38ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english_1\n",
      "Train Size:6400\n",
      "Train Size:1600\n",
      "\n",
      "filipino\n",
      "Train Size:6400\n",
      "Train Size:1600\n",
      "\n",
      "chinese\n",
      "Train Size:6400\n",
      "Train Size:1600\n",
      "\n",
      "korean\n",
      "Train Size:6400\n",
      "Train Size:1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = {'feature_level':'word',\n",
    "        'clf':'LR',\n",
    "        'language':None,\n",
    "        'ngram_range':(1,3),\n",
    "        'max_features':14000,\n",
    "        \n",
    "       }\n",
    "\n",
    "for lang in languages:\n",
    "\n",
    "    with open(f\"processed_data/{lang}_processed.pkl\", \"rb\") as f:\n",
    "            _data = pickle.load(f)\n",
    "    print(lang)\n",
    "    print(f'Train Size:{len(_data[\"text_train\"])}')\n",
    "    print(f'Train Size:{len(_data[\"text_test\"])}')\n",
    "    print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c7b52c",
   "metadata": {},
   "source": [
    "# Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eb5cbfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3361253781b64f639cc705c0eae4cc08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.84      1.00      0.91      1328\n",
      "not_offensive       1.00      0.08      0.14       272\n",
      "\n",
      "     accuracy                           0.84      1600\n",
      "    macro avg       0.92      0.54      0.53      1600\n",
      " weighted avg       0.87      0.84      0.78      1600\n",
      "\n",
      "======================\n",
      "LR\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.92      0.98      0.95      1328\n",
      "not_offensive       0.89      0.58      0.70       272\n",
      "\n",
      "     accuracy                           0.92      1600\n",
      "    macro avg       0.90      0.78      0.83      1600\n",
      " weighted avg       0.91      0.92      0.91      1600\n",
      "\n",
      "======================\n",
      "SVM\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.91      0.99      0.95      1328\n",
      "not_offensive       0.91      0.53      0.67       272\n",
      "\n",
      "     accuracy                           0.91      1600\n",
      "    macro avg       0.91      0.76      0.81      1600\n",
      " weighted avg       0.91      0.91      0.90      1600\n",
      "\n",
      "======================\n",
      "RF\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.92      0.98      0.95      1328\n",
      "not_offensive       0.86      0.57      0.68       272\n",
      "\n",
      "     accuracy                           0.91      1600\n",
      "    macro avg       0.89      0.77      0.81      1600\n",
      " weighted avg       0.91      0.91      0.90      1600\n",
      "\n",
      "======================\n",
      "GBT\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.95      0.96      0.96      1328\n",
      "not_offensive       0.80      0.77      0.78       272\n",
      "\n",
      "     accuracy                           0.93      1600\n",
      "    macro avg       0.87      0.87      0.87      1600\n",
      " weighted avg       0.93      0.93      0.93      1600\n",
      "\n",
      "======================\n",
      "filipino\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96febb00391a4b9382f50eeaf14895da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.72      0.75      0.73       747\n",
      "not_offensive       0.77      0.74      0.76       853\n",
      "\n",
      "     accuracy                           0.75      1600\n",
      "    macro avg       0.75      0.75      0.75      1600\n",
      " weighted avg       0.75      0.75      0.75      1600\n",
      "\n",
      "======================\n",
      "LR\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.78      0.75      0.77       747\n",
      "not_offensive       0.79      0.82      0.80       853\n",
      "\n",
      "     accuracy                           0.79      1600\n",
      "    macro avg       0.79      0.78      0.78      1600\n",
      " weighted avg       0.79      0.79      0.79      1600\n",
      "\n",
      "======================\n",
      "SVM\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.77      0.75      0.76       747\n",
      "not_offensive       0.79      0.81      0.80       853\n",
      "\n",
      "     accuracy                           0.78      1600\n",
      "    macro avg       0.78      0.78      0.78      1600\n",
      " weighted avg       0.78      0.78      0.78      1600\n",
      "\n",
      "======================\n",
      "RF\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.77      0.76      0.76       747\n",
      "not_offensive       0.79      0.80      0.79       853\n",
      "\n",
      "     accuracy                           0.78      1600\n",
      "    macro avg       0.78      0.78      0.78      1600\n",
      " weighted avg       0.78      0.78      0.78      1600\n",
      "\n",
      "======================\n",
      "GBT\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.69      0.71      0.70       747\n",
      "not_offensive       0.74      0.72      0.73       853\n",
      "\n",
      "     accuracy                           0.71      1600\n",
      "    macro avg       0.71      0.71      0.71      1600\n",
      " weighted avg       0.71      0.71      0.71      1600\n",
      "\n",
      "======================\n",
      "chinese\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d75ee97f55410badbf58c5da7b17ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.77      0.23      0.35       555\n",
      "not_offensive       0.70      0.96      0.81      1045\n",
      "\n",
      "     accuracy                           0.71      1600\n",
      "    macro avg       0.74      0.60      0.58      1600\n",
      " weighted avg       0.73      0.71      0.65      1600\n",
      "\n",
      "======================\n",
      "LR\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.72      0.56      0.63       555\n",
      "not_offensive       0.79      0.88      0.83      1045\n",
      "\n",
      "     accuracy                           0.77      1600\n",
      "    macro avg       0.75      0.72      0.73      1600\n",
      " weighted avg       0.77      0.77      0.76      1600\n",
      "\n",
      "======================\n",
      "SVM\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.72      0.54      0.62       555\n",
      "not_offensive       0.78      0.89      0.83      1045\n",
      "\n",
      "     accuracy                           0.77      1600\n",
      "    macro avg       0.75      0.71      0.72      1600\n",
      " weighted avg       0.76      0.77      0.76      1600\n",
      "\n",
      "======================\n",
      "RF\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.64      0.56      0.60       555\n",
      "not_offensive       0.78      0.83      0.81      1045\n",
      "\n",
      "     accuracy                           0.74      1600\n",
      "    macro avg       0.71      0.70      0.70      1600\n",
      " weighted avg       0.73      0.74      0.73      1600\n",
      "\n",
      "======================\n",
      "GBT\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.67      0.56      0.61       555\n",
      "not_offensive       0.78      0.85      0.82      1045\n",
      "\n",
      "     accuracy                           0.75      1600\n",
      "    macro avg       0.73      0.71      0.71      1600\n",
      " weighted avg       0.74      0.75      0.75      1600\n",
      "\n",
      "======================\n",
      "korean\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1cee17d3bf4f45a67dbc469f3a1fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "not_offensive       0.73      0.39      0.51       697\n",
      "    offensive       0.66      0.89      0.75       903\n",
      "\n",
      "     accuracy                           0.67      1600\n",
      "    macro avg       0.69      0.64      0.63      1600\n",
      " weighted avg       0.69      0.67      0.65      1600\n",
      "\n",
      "======================\n",
      "LR\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "not_offensive       0.65      0.50      0.56       697\n",
      "    offensive       0.67      0.80      0.73       903\n",
      "\n",
      "     accuracy                           0.67      1600\n",
      "    macro avg       0.66      0.65      0.65      1600\n",
      " weighted avg       0.66      0.67      0.66      1600\n",
      "\n",
      "======================\n",
      "SVM\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "not_offensive       0.67      0.48      0.56       697\n",
      "    offensive       0.67      0.82      0.74       903\n",
      "\n",
      "     accuracy                           0.67      1600\n",
      "    macro avg       0.67      0.65      0.65      1600\n",
      " weighted avg       0.67      0.67      0.66      1600\n",
      "\n",
      "======================\n",
      "RF\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "not_offensive       0.63      0.47      0.54       697\n",
      "    offensive       0.66      0.79      0.72       903\n",
      "\n",
      "     accuracy                           0.65      1600\n",
      "    macro avg       0.65      0.63      0.63      1600\n",
      " weighted avg       0.65      0.65      0.64      1600\n",
      "\n",
      "======================\n",
      "GBT\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "not_offensive       0.63      0.35      0.45       697\n",
      "    offensive       0.63      0.84      0.72       903\n",
      "\n",
      "     accuracy                           0.63      1600\n",
      "    macro avg       0.63      0.60      0.58      1600\n",
      " weighted avg       0.63      0.63      0.60      1600\n",
      "\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "args = {'feature_level':'word',\n",
    "        'clf':'LR',\n",
    "        'language':None,\n",
    "        'ngram_range':(1,3),\n",
    "        'max_features':14000,\n",
    "        \n",
    "       }\n",
    "\n",
    "for lang in languages:\n",
    "    args['language'] = lang\n",
    "\n",
    "    print(args['language'])\n",
    "    for m in tqdm(models):\n",
    "\n",
    "        with open(f\"processed_data/{args['language']}_processed.pkl\", \"rb\") as f:\n",
    "            _data = pickle.load(f)\n",
    "\n",
    "        clf = train(m,args['ngram_range'],args['feature_level'],args['max_features'],_data,list(_data['label_dict'].keys()))\n",
    "\n",
    "        model_name = args['language']+'_'+m+'_'+args[\"feature_level\"]+'.joblib'\n",
    "        dump(clf, f'output/{model_name}') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14845eda",
   "metadata": {},
   "source": [
    "# Char Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dcfef3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf6d775113148bbb8692aa75eacc072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.83      1.00      0.91      1328\n",
      "not_offensive       0.77      0.04      0.07       272\n",
      "\n",
      "     accuracy                           0.83      1600\n",
      "    macro avg       0.80      0.52      0.49      1600\n",
      " weighted avg       0.82      0.83      0.77      1600\n",
      "\n",
      "======================\n",
      "LR\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.93      0.98      0.95      1328\n",
      "not_offensive       0.86      0.65      0.74       272\n",
      "\n",
      "     accuracy                           0.92      1600\n",
      "    macro avg       0.90      0.82      0.85      1600\n",
      " weighted avg       0.92      0.92      0.92      1600\n",
      "\n",
      "======================\n",
      "SVM\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.92      0.98      0.95      1328\n",
      "not_offensive       0.85      0.61      0.71       272\n",
      "\n",
      "     accuracy                           0.92      1600\n",
      "    macro avg       0.89      0.79      0.83      1600\n",
      " weighted avg       0.91      0.92      0.91      1600\n",
      "\n",
      "======================\n",
      "RF\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.90      0.99      0.94      1328\n",
      "not_offensive       0.88      0.49      0.63       272\n",
      "\n",
      "     accuracy                           0.90      1600\n",
      "    macro avg       0.89      0.74      0.79      1600\n",
      " weighted avg       0.90      0.90      0.89      1600\n",
      "\n",
      "======================\n",
      "GBT\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.95      0.96      0.95      1328\n",
      "not_offensive       0.79      0.75      0.77       272\n",
      "\n",
      "     accuracy                           0.92      1600\n",
      "    macro avg       0.87      0.86      0.86      1600\n",
      " weighted avg       0.92      0.92      0.92      1600\n",
      "\n",
      "======================\n",
      "filipino\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e6eb5fd75f40119a8d216b269ee41f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.67      0.73      0.70       747\n",
      "not_offensive       0.74      0.69      0.72       853\n",
      "\n",
      "     accuracy                           0.71      1600\n",
      "    macro avg       0.71      0.71      0.71      1600\n",
      " weighted avg       0.71      0.71      0.71      1600\n",
      "\n",
      "======================\n",
      "LR\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.76      0.74      0.75       747\n",
      "not_offensive       0.78      0.80      0.79       853\n",
      "\n",
      "     accuracy                           0.77      1600\n",
      "    macro avg       0.77      0.77      0.77      1600\n",
      " weighted avg       0.77      0.77      0.77      1600\n",
      "\n",
      "======================\n",
      "SVM\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.77      0.72      0.75       747\n",
      "not_offensive       0.77      0.81      0.79       853\n",
      "\n",
      "     accuracy                           0.77      1600\n",
      "    macro avg       0.77      0.77      0.77      1600\n",
      " weighted avg       0.77      0.77      0.77      1600\n",
      "\n",
      "======================\n",
      "RF\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.77      0.78      0.78       747\n",
      "not_offensive       0.81      0.80      0.80       853\n",
      "\n",
      "     accuracy                           0.79      1600\n",
      "    macro avg       0.79      0.79      0.79      1600\n",
      " weighted avg       0.79      0.79      0.79      1600\n",
      "\n",
      "======================\n",
      "GBT\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.72      0.72      0.72       747\n",
      "not_offensive       0.75      0.76      0.76       853\n",
      "\n",
      "     accuracy                           0.74      1600\n",
      "    macro avg       0.74      0.74      0.74      1600\n",
      " weighted avg       0.74      0.74      0.74      1600\n",
      "\n",
      "======================\n",
      "chinese\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922431dd530542b5b444c9466c1985d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.86      0.10      0.18       555\n",
      "not_offensive       0.67      0.99      0.80      1045\n",
      "\n",
      "     accuracy                           0.68      1600\n",
      "    macro avg       0.77      0.55      0.49      1600\n",
      " weighted avg       0.74      0.68      0.59      1600\n",
      "\n",
      "======================\n",
      "LR\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.74      0.55      0.63       555\n",
      "not_offensive       0.79      0.90      0.84      1045\n",
      "\n",
      "     accuracy                           0.78      1600\n",
      "    macro avg       0.77      0.72      0.74      1600\n",
      " weighted avg       0.77      0.78      0.77      1600\n",
      "\n",
      "======================\n",
      "SVM\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.76      0.52      0.62       555\n",
      "not_offensive       0.78      0.91      0.84      1045\n",
      "\n",
      "     accuracy                           0.78      1600\n",
      "    macro avg       0.77      0.72      0.73      1600\n",
      " weighted avg       0.78      0.78      0.77      1600\n",
      "\n",
      "======================\n",
      "RF\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.74      0.58      0.65       555\n",
      "not_offensive       0.80      0.89      0.84      1045\n",
      "\n",
      "     accuracy                           0.78      1600\n",
      "    macro avg       0.77      0.74      0.75      1600\n",
      " weighted avg       0.78      0.78      0.78      1600\n",
      "\n",
      "======================\n",
      "GBT\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    offensive       0.66      0.58      0.62       555\n",
      "not_offensive       0.79      0.84      0.81      1045\n",
      "\n",
      "     accuracy                           0.75      1600\n",
      "    macro avg       0.72      0.71      0.72      1600\n",
      " weighted avg       0.74      0.75      0.75      1600\n",
      "\n",
      "======================\n",
      "korean\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14761f60ab2f4e698501ed25f86c5f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "not_offensive       0.88      0.31      0.46       697\n",
      "    offensive       0.64      0.97      0.77       903\n",
      "\n",
      "     accuracy                           0.68      1600\n",
      "    macro avg       0.76      0.64      0.61      1600\n",
      " weighted avg       0.75      0.68      0.64      1600\n",
      "\n",
      "======================\n",
      "LR\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "not_offensive       0.75      0.62      0.68       697\n",
      "    offensive       0.74      0.84      0.79       903\n",
      "\n",
      "     accuracy                           0.74      1600\n",
      "    macro avg       0.74      0.73      0.73      1600\n",
      " weighted avg       0.74      0.74      0.74      1600\n",
      "\n",
      "======================\n",
      "SVM\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "not_offensive       0.73      0.62      0.67       697\n",
      "    offensive       0.74      0.82      0.78       903\n",
      "\n",
      "     accuracy                           0.74      1600\n",
      "    macro avg       0.74      0.72      0.73      1600\n",
      " weighted avg       0.74      0.74      0.73      1600\n",
      "\n",
      "======================\n",
      "RF\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "not_offensive       0.72      0.53      0.61       697\n",
      "    offensive       0.70      0.84      0.76       903\n",
      "\n",
      "     accuracy                           0.70      1600\n",
      "    macro avg       0.71      0.68      0.69      1600\n",
      " weighted avg       0.71      0.70      0.70      1600\n",
      "\n",
      "======================\n",
      "GBT\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "not_offensive       0.63      0.61      0.62       697\n",
      "    offensive       0.71      0.72      0.71       903\n",
      "\n",
      "     accuracy                           0.67      1600\n",
      "    macro avg       0.67      0.67      0.67      1600\n",
      " weighted avg       0.67      0.67      0.67      1600\n",
      "\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "args = {'feature_level':'char',\n",
    "        'clf':'LR',\n",
    "        'language':None,\n",
    "        'ngram_range':(1,3),\n",
    "        'max_features':53000,\n",
    "        \n",
    "       }\n",
    "\n",
    "for lang in languages:\n",
    "    args['language'] = lang\n",
    "\n",
    "    print(args['language'])\n",
    "    for m in tqdm(models):\n",
    "\n",
    "        with open(f\"processed_data/{args['language']}_processed.pkl\", \"rb\") as f:\n",
    "            _data = pickle.load(f)\n",
    "\n",
    "        clf = train(m,args['ngram_range'],args['feature_level'],args['max_features'],_data,list(_data['label_dict'].keys()))\n",
    "\n",
    "        model_name = args['language']+'_'+m+'_'+args[\"feature_level\"]+'.joblib'\n",
    "        dump(clf, f'output/{model_name}') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eef68c",
   "metadata": {},
   "source": [
    "# Next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9456377b",
   "metadata": {},
   "source": [
    "- Implement Preprocessing Pipeline - Done\n",
    "- Transform and  new english dataset - Done\n",
    "- Work on CNN/RNN - To Do\n",
    "- Work on CNN/RNN with encodings - To Do\n",
    "- Work on Bert - To Do\n",
    "- Test if translating languages and increasing dataset size helps - To Do"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
